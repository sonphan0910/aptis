
// C·∫•u h√¨nh AI - D√πng Google Gemini > Groq > Fallback
require('dotenv').config();

// ========================================
// GOOGLE GEMINI CONFIGURATION (Recommended) üéØ
// ========================================
const GEMINI_CONFIG = {
  apiKey: process.env.GOOGLE_GEMINI_API_KEY,
  model: process.env.GOOGLE_GEMINI_MODEL || 'gemini-2.0-flash-exp', // Most accurate & fastest
  baseURL: 'https://generativelanguage.googleapis.com/v1beta/models',
  temperature: 0.3, // Lower = more consistent, deterministic
  maxTokens: 1024, // Enough for scoring JSON responses
};

// ========================================
// GROQ CONFIGURATION (Cloud) - Fallback
// ========================================
const GROQ_CONFIG = {
  apiKey: process.env.GROQ_API_KEY,
  model: process.env.GROQ_MODEL || 'mixtral-8x7b-32768',
  baseURL: 'https://api.groq.com/openai/v1',
  temperature: 0.3,
  maxTokens: 1024,
};

// Determine which AI provider to use
let useGemini = !!GEMINI_CONFIG.apiKey;
let useGroq = !!GROQ_CONFIG.apiKey && !useGemini;

// Ki·ªÉm tra AI provider kh·∫£ d·ª•ng
const checkAIProviders = async () => {
  if (useGemini) {
    console.log('[AI Config] ‚úÖ Google Gemini API configured');
    console.log('[AI Config] ü§ñ Model: ' + GEMINI_CONFIG.model);
    return;
  }

  if (useGroq) {
    console.log('[AI Config] ‚úÖ Groq API configured (Fallback)');
    console.log('[AI Config] ü§ñ Model: ' + GROQ_CONFIG.model);
    return;
  }

  console.log('[AI Config] ‚ùå No AI provider available');
  console.log('[AI Config] üí° H√£y thi·∫øt l·∫≠p GOOGLE_GEMINI_API_KEY ho·∫∑c GROQ_API_KEY');
};

// Check providers on startup
checkAIProviders();

/**
 * G·ªçi Google Gemini API (Cloud-based, most accurate)
 * @param {string} prompt - Prompt ƒë·ªÉ g·ª≠i
 * @returns {Promise<string>} K·∫øt qu·∫£ t·ª´ model
 */
const callGemini = async (prompt) => {
  if (!GEMINI_CONFIG.apiKey) {
    throw new Error('GOOGLE_GEMINI_API_KEY not configured');
  }

  try {
    console.log(`[Gemini] Calling model: ${GEMINI_CONFIG.model}`);
    
    const response = await fetch(
      `${GEMINI_CONFIG.baseURL}/${GEMINI_CONFIG.model}:generateContent?key=${GEMINI_CONFIG.apiKey}`,
      {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          contents: [
            {
              parts: [
                {
                  text: prompt
                }
              ]
            }
          ],
          generationConfig: {
            temperature: GEMINI_CONFIG.temperature,
            maxOutputTokens: GEMINI_CONFIG.maxTokens,
            topP: 0.95,
            topK: 40,
          }
        }),
        timeout: 300000 // 5 minute timeout
      }
    );

    if (!response.ok) {
      const errorData = await response.json();
      const errorMsg = errorData?.error?.message || `HTTP ${response.status}`;
      throw new Error(`Gemini API error: ${errorMsg}`);
    }

    const data = await response.json();
    
    if (!data.candidates || data.candidates.length === 0) {
      throw new Error('No response from Gemini');
    }

    const content = data.candidates[0]?.content?.parts?.[0]?.text || '';
    console.log('[Gemini] ‚úÖ Response received');
    return content;
  } catch (error) {
    console.error('[Gemini] ‚ùå Error:', error.message);
    throw error;
  }
};

/**
 * G·ªçi Groq API (Cloud-based, very fast)
 * @param {string} prompt - Prompt ƒë·ªÉ g·ª≠i
 * @returns {Promise<string>} K·∫øt qu·∫£ t·ª´ model
 */
const callGroq = async (prompt) => {
  if (!GROQ_CONFIG.apiKey) {
    throw new Error('GROQ_API_KEY not configured');
  }

  try {
    console.log(`[Groq] Calling model: ${GROQ_CONFIG.model}`);
    
    const response = await fetch(`${GROQ_CONFIG.baseURL}/chat/completions`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${GROQ_CONFIG.apiKey}`
      },
      body: JSON.stringify({
        model: GROQ_CONFIG.model,
        messages: [
          {
            role: 'user',
            content: prompt
          }
        ],
        temperature: GROQ_CONFIG.temperature,
        max_tokens: GROQ_CONFIG.maxTokens,
      }),
      timeout: 300000 // 5 minute timeout
    });

    if (!response.ok) {
      const errorData = await response.json();
      const errorMsg = errorData?.error?.message || `HTTP ${response.status}`;
      throw new Error(`Groq API error: ${errorMsg}`);
    }

    const data = await response.json();
    const content = data.choices?.[0]?.message?.content || '';
    console.log('[Groq] ‚úÖ Response received');
    return content;
  } catch (error) {
    console.error('[Groq] ‚ùå Error:', error.message);
    throw error;
  }
};

/**
 * G·ªçi Ollama API (local inference)
 * @param {string} prompt - Prompt ƒë·ªÉ g·ª≠i t·ªõi model
 * @param {object} options - T√πy ch·ªçn (model, temperature, v.v.)
 * @returns {Promise<string>} K·∫øt qu·∫£ t·ª´ model
 */
const callOllama = async (prompt, options = {}) => {
  const model = options.model || OLLAMA_CONFIG.model;
  const temperature = options.temperature ?? OLLAMA_CONFIG.temperature;
  
  try {
    console.log(`[Ollama] Calling model: ${model}`);
    const response = await fetch(`${OLLAMA_CONFIG.baseURL}/api/generate`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        model,
        prompt,
        stream: false,
        options: {
          temperature,
          num_predict: options.max_tokens || OLLAMA_CONFIG.num_predict,
        }
      }),
      timeout: 300000 // 5 minute timeout for local inference
    });

    if (!response.ok) {
      const errorText = await response.text();
      throw new Error(`Ollama API error: ${response.status} - ${errorText}`);
    }

    const data = await response.json();
    console.log('[Ollama] ‚úÖ Response received');
    return data.response || '';
  } catch (error) {
    console.error('[Ollama] ‚ùå Error:', error.message);
    throw error;
  }
};



/**
 * G·ªçi AI service (Gemini > Groq fallback)
 * @param {string} prompt - Prompt ƒë·ªÉ g·ª≠i
 * @param {object} options - T√πy ch·ªçn
 * @returns {Promise<string>} K·∫øt qu·∫£ t·ª´ model
 */
const callAI = async (prompt, options = {}) => {
  if (useGemini) {
    console.log('[AI] Calling Google Gemini...');
    return callGemini(prompt);
  } else if (useGroq) {
    console.log('[AI] Calling Groq (fallback)...');
    return callGroq(prompt);
  } else {
    throw new Error('No AI provider available. Please configure GOOGLE_GEMINI_API_KEY or GROQ_API_KEY.');
  }
};

// Export c√°c th√†nh ph·∫ßn c·∫•u h√¨nh AI
module.exports = {
  // Gemini (Cloud)
  callGemini,
  GEMINI_CONFIG,
  
  // Groq (Cloud fallback)
  callGroq,
  GROQ_CONFIG,
  
  // AI provider chung (Gemini > Groq)
  callAI,
  checkAIProviders,
  
  // Status
  isUsingGemini: () => useGemini,
  isUsingGroq: () => useGroq,
};
