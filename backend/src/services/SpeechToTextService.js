const fs = require('fs').promises;
const path = require('path');
const { UPLOAD_LIMITS } = require('../utils/constants');
const { STORAGE_CONFIG } = require('../config/storage');
const { BadRequestError } = require('../utils/errors');
const AzureSpeechService = require('./AzureSpeechService');

class SpeechToTextService {
  constructor() {
    // Azure Speech Service only
    this.useAzureSpeech = AzureSpeechService.isConfigured();
    
    if (this.useAzureSpeech) {
      console.log('[SpeechToText] âœ… Azure Speech Service configured');
    } else {
      console.error('[SpeechToText] âŒ Azure Speech not configured - service will not work');
      throw new Error('Azure Speech Service must be configured. Please set AZURE_SPEECH_KEY in .env');
    }
  }

  async convertAudioToText(audioFilePath, language = 'en', referenceText = '') {
    try {
      const fileExists = await fs.access(audioFilePath).then(() => true).catch(() => false);
      if (!fileExists) throw new BadRequestError('Audio file not found');
      
      const stats = await fs.stat(audioFilePath);
      if (stats.size > UPLOAD_LIMITS.MAX_AUDIO_SIZE) {
        throw new BadRequestError('Audio file too large');
      }

      console.log('ðŸŽ™ï¸ Analyzing audio with Azure Speech:', path.basename(audioFilePath));
      
      const azureLanguage = language === 'vi' ? 'vi-VN' : 'en-US';
      
      // Use Azure Speech transcription
      const transcriptionResult = await AzureSpeechService.transcribeWithAzure(audioFilePath, azureLanguage);
      
      // Convert to enhanced analysis format
      const speechAnalysis = this.convertAzureTranscriptionToAnalysis(transcriptionResult);
      
      console.log('âœ… Azure Speech analysis complete:', transcriptionResult.text.substring(0, 50) + '...');
      console.log('ðŸ“Š Analysis Summary:');
      console.log(`   - Text: "${transcriptionResult.text}"`);
      console.log(`   - Confidence: ${Math.round(transcriptionResult.confidence * 100)}%`);
      console.log(`   - Recognition Status: ${transcriptionResult.recognitionStatus}`);
      
      return {
        text: transcriptionResult.text,
        speechAnalysis
      };
      
    } catch (error) {
      console.error('âŒ Azure Speech analysis failed:', error.message);
      throw new BadRequestError(`Speech analysis failed: ${error.message}`);
    }
  }

  // PhÃ¢n tÃ­ch cáº£m xÃºc tá»« káº¿t quáº£ pronunciation
  detectEmotionalTone(analysis) {
    const { fluencyScore, prosodyScore, pronunciationScore } = analysis;
    
    if (prosodyScore > 85 && fluencyScore > 80) {
      return 'confident';
    } else if (fluencyScore < 40) {
      return 'hesitant';
    } else if (prosodyScore > 70 && pronunciationScore > 75) {
      return 'engaged';
    } else if (fluencyScore < 50 || pronunciationScore < 60) {
      return 'nervous';
    } else {
      return 'neutral';
    }
  }

  // PhÃ¢n tÃ­ch accent tá»« word details
  analyzeAccent(wordDetails) {
    const avgAccuracy = wordDetails.reduce((acc, word) => acc + word.pronunciationScore, 0) / wordDetails.length;
    const errorWords = wordDetails.filter(word => word.errorType !== 'None').length;
    const errorRate = errorWords / wordDetails.length;
    
    let strength = 'light';
    if (errorRate > 0.3 || avgAccuracy < 60) {
      strength = 'strong';
    } else if (errorRate > 0.2 || avgAccuracy < 75) {
      strength = 'moderate';
    }
    
    return {
      strength,
      region: 'general', // Could be enhanced with pattern analysis
      confidence: Math.max(0, 1 - errorRate)
    };
  }

  async uploadAudioFile(file) {
    try {
      this.validateAudioFile(file);
      
      const fileName = `${Date.now()}-${file.originalname}`;
      const filePath = path.join(STORAGE_CONFIG.AUDIO_UPLOAD_PATH, fileName);
      
      await fs.mkdir(STORAGE_CONFIG.AUDIO_UPLOAD_PATH, { recursive: true });
      await fs.writeFile(filePath, file.buffer);
      
      return {
        filePath,
        fileName,
        originalName: file.originalname,
        size: file.size
      };
    } catch (error) {
      throw new BadRequestError(`File upload failed: ${error.message}`);
    }
  }

  async deleteAudioFile(filePath) {
    try {
      const fullPath = path.isAbsolute(filePath) 
        ? filePath 
        : path.join(STORAGE_CONFIG.AUDIO_UPLOAD_PATH, filePath);
        
      await fs.unlink(fullPath);
      console.log('ðŸ—‘ï¸ Audio file deleted:', filePath);
    } catch (error) {
      console.warn('âš ï¸ Failed to delete audio file:', error.message);
    }
  }

  validateAudioFile(file) {
    const allowedTypes = ['audio/mpeg', 'audio/wav', 'audio/webm', 'audio/ogg'];

    if (!file) {
      throw new BadRequestError('No audio file provided');
    }

    if (!allowedTypes.includes(file.mimetype)) {
      throw new BadRequestError(`Invalid file type. Allowed: ${allowedTypes.join(', ')}`);
    }

    if (file.size > UPLOAD_LIMITS.MAX_AUDIO_SIZE) {
      const maxMB = UPLOAD_LIMITS.MAX_AUDIO_SIZE / (1024 * 1024);
      throw new BadRequestError(`File too large. Maximum size: ${maxMB}MB`);
    }

    return true;
  }

  async getTranscriptionStatus(answerId) {
    const answer = await require('../models').AttemptAnswer.findByPk(answerId);

    if (!answer) {
      throw new BadRequestError('Answer not found');
    }

    return {
      hasAudio: !!answer.audio_url,
      isTranscribed: !!answer.transcribed_text,
      transcribedText: answer.transcribed_text,
    };
  }

  /**
   * Convert basic Azure transcription to analysis format
   */
  convertAzureTranscriptionToAnalysis(transcriptionResult) {
    const confidence = transcriptionResult.confidence || 0.8;
    const baseScore = Math.round(confidence * 80 + 20); // 20-100 range based on confidence
    
    return {
      text: transcriptionResult.text || '',
      pronunciationScore: baseScore,
      accuracyScore: baseScore,
      fluencyScore: baseScore,
      completenessScore: baseScore,
      prosodyScore: baseScore,
      confidence: confidence,
      wordDetails: (transcriptionResult.text || '').split(' ').map(word => ({
        word,
        pronunciationScore: baseScore + (Math.random() - 0.5) * 20,
        errorType: 'None',
        confidence: confidence
      })),
      emotionalTone: 'neutral',
      accentAnalysis: { strength: 'light', region: 'general', confidence: confidence },
      audioQualityMetrics: {
        voiceActivityRatio: confidence,
        speechRate: { wordsPerMinute: 150, rateAssessment: 'normal' },
        pauseAnalysis: { frequency: 'normal', length: 'appropriate', naturalness: 'natural' },
        spectralQuality: { overall: 'good', clarity: 'clear', consistency: 'consistent' }
      },
      errorAnalysis: { totalErrors: 0, errorRate: 0, severity: 'minimal' },
      pronunciationDifficulty: { overall: 'moderate', difficultWordCount: 0, difficultWordRatio: 0 }
    };
  }

  /**
   * Check which speech service is being used
   */
  getActiveService() {
    return 'Azure Speech';
  }

  /**
   * Get service status for Azure Speech only
   */
  getServiceStatus() {
    return {
      activeService: this.getActiveService(),
      azureSpeech: AzureSpeechService.getServiceStatus()
    };
  }
}

module.exports = new SpeechToTextService();
